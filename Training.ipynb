{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPFS2USVRzjZC1lbm5OYKM7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":21,"metadata":{"id":"2o_LOUgptxqf","executionInfo":{"status":"ok","timestamp":1687971988961,"user_tz":-330,"elapsed":642,"user":{"displayName":"Gaja Lakshmi Gedala","userId":"08948831165546628882"}}},"outputs":[],"source":["from keras.layers import Input, Lambda, Dense, Flatten\n","from keras.models import Model\n","from keras.applications.vgg16 import VGG16\n","from keras.applications.vgg16 import preprocess_input\n","from keras.preprocessing import image\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","import numpy as np\n","from glob import glob\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)"]},{"cell_type":"code","source":["IMAGE_SIZE = [224, 224]"],"metadata":{"id":"l-YQDP5luK6A","executionInfo":{"status":"ok","timestamp":1687971992208,"user_tz":-330,"elapsed":432,"user":{"displayName":"Gaja Lakshmi Gedala","userId":"08948831165546628882"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j18U-UvIvaDV","executionInfo":{"status":"ok","timestamp":1687971997993,"user_tz":-330,"elapsed":3551,"user":{"displayName":"Gaja Lakshmi Gedala","userId":"08948831165546628882"}},"outputId":"6b097a34-4654-4413-e27b-4afa5580bf1c"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["train_path = '/content/drive/MyDrive/Dataset'\n","test_path = '/content/drive/MyDrive/test'"],"metadata":{"id":"CzsYwKgtwG2S","executionInfo":{"status":"ok","timestamp":1687971999719,"user_tz":-330,"elapsed":4,"user":{"displayName":"Gaja Lakshmi Gedala","userId":"08948831165546628882"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n","\n","for layer in vgg.layers:\n","    layer.trainable = False"],"metadata":{"id":"-N7m4YRpwUcS","executionInfo":{"status":"ok","timestamp":1687972003652,"user_tz":-330,"elapsed":500,"user":{"displayName":"Gaja Lakshmi Gedala","userId":"08948831165546628882"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["folders = glob('/content/drive/MyDrive/Dataset/*')"],"metadata":{"id":"cfSPqt6-wWAZ","executionInfo":{"status":"ok","timestamp":1687972008247,"user_tz":-330,"elapsed":440,"user":{"displayName":"Gaja Lakshmi Gedala","userId":"08948831165546628882"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["x = Flatten()(vgg.output)"],"metadata":{"id":"6bDBA5Lpwe3q","executionInfo":{"status":"ok","timestamp":1687972011793,"user_tz":-330,"elapsed":666,"user":{"displayName":"Gaja Lakshmi Gedala","userId":"08948831165546628882"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["prediction = Dense(len(folders), activation='softmax')(x)"],"metadata":{"id":"LxgQ8QMNwhwJ","executionInfo":{"status":"ok","timestamp":1687972014156,"user_tz":-330,"elapsed":444,"user":{"displayName":"Gaja Lakshmi Gedala","userId":"08948831165546628882"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["model = Model(inputs = vgg.input, outputs = prediction)"],"metadata":{"id":"sUrO1_1VwlHR","executionInfo":{"status":"ok","timestamp":1687972019037,"user_tz":-330,"elapsed":419,"user":{"displayName":"Gaja Lakshmi Gedala","userId":"08948831165546628882"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v-HYBI_MwpPA","executionInfo":{"status":"ok","timestamp":1687972021548,"user_tz":-330,"elapsed":514,"user":{"displayName":"Gaja Lakshmi Gedala","userId":"08948831165546628882"}},"outputId":"ccfc8dc5-8e79-4eae-db6a-d81540966f9e"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 25088)             0         \n","                                                                 \n"," dense_1 (Dense)             (None, 2)                 50178     \n","                                                                 \n","=================================================================\n","Total params: 14,764,866\n","Trainable params: 50,178\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.compile(\n","    loss = 'binary_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy']\n",")"],"metadata":{"id":"aP87jgrMwtua","executionInfo":{"status":"ok","timestamp":1687972024956,"user_tz":-330,"elapsed":411,"user":{"displayName":"Gaja Lakshmi Gedala","userId":"08948831165546628882"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","train_datagen = ImageDataGenerator(rescale = 1./255,\n","                                  shear_range = 0.2,\n","                                  zoom_range = 0.2,\n","                                  horizontal_flip = True)\n","\n","test_datagen = ImageDataGenerator(rescale = 1./255,\n","                                  shear_range = 0.2,\n","                                  zoom_range = 0.2)\n","\n","training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/Dataset',\n","                                                target_size = (224, 224),\n","                                                batch_size = 32,\n","                                                class_mode = 'categorical')\n","\n","test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/test',\n","                                           target_size = (224,224),\n","                                           batch_size = 32,\n","                                           class_mode = 'categorical')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E9IJ3MdAwx73","executionInfo":{"status":"ok","timestamp":1687972027559,"user_tz":-330,"elapsed":426,"user":{"displayName":"Gaja Lakshmi Gedala","userId":"08948831165546628882"}},"outputId":"1070a329-62da-4bbb-dee4-04816117bd23"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 8823 images belonging to 2 classes.\n","Found 946 images belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["from datetime import datetime\n","from keras.callbacks import ModelCheckpoint\n","\n","checkpoint = ModelCheckpoint(filepath='mymodel2.h5', verbose = 2, save_best_only=True)\n","\n","callbac = [checkpoint]\n","\n","start = datetime.now()\n","\n","model_history = model.fit(\n","    training_set,\n","    validation_data = test_set,\n","    epochs = 50,\n","    steps_per_epoch = len(training_set),\n","    validation_steps = len(test_set),\n","    callbacks=callbac, verbose=2\n","    )\n","\n","duration = datetime.now() - start\n","print(\"Time taken;\", duration)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"biDo774Tw563","executionInfo":{"status":"error","timestamp":1687984393414,"user_tz":-330,"elapsed":12362227,"user":{"displayName":"Gaja Lakshmi Gedala","userId":"08948831165546628882"}},"outputId":"3079aaf2-173f-445a-db1d-cb687ae4954f"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","\n","Epoch 1: val_loss improved from inf to 0.31554, saving model to mymodel2.h5\n","276/276 - 1205s - loss: 0.3912 - accuracy: 0.8244 - val_loss: 0.3155 - val_accuracy: 0.8700 - 1205s/epoch - 4s/step\n","Epoch 2/50\n","\n","Epoch 2: val_loss did not improve from 0.31554\n","276/276 - 215s - loss: 0.2845 - accuracy: 0.8786 - val_loss: 0.3518 - val_accuracy: 0.8457 - 215s/epoch - 780ms/step\n","Epoch 3/50\n","\n","Epoch 3: val_loss did not improve from 0.31554\n","276/276 - 238s - loss: 0.2415 - accuracy: 0.9017 - val_loss: 0.3216 - val_accuracy: 0.8636 - 238s/epoch - 861ms/step\n","Epoch 4/50\n","\n","Epoch 4: val_loss did not improve from 0.31554\n","276/276 - 220s - loss: 0.2171 - accuracy: 0.9127 - val_loss: 0.3748 - val_accuracy: 0.8362 - 220s/epoch - 798ms/step\n","Epoch 5/50\n","\n","Epoch 5: val_loss did not improve from 0.31554\n","276/276 - 220s - loss: 0.2028 - accuracy: 0.9191 - val_loss: 0.6493 - val_accuracy: 0.7622 - 220s/epoch - 799ms/step\n","Epoch 6/50\n","\n","Epoch 6: val_loss improved from 0.31554 to 0.29840, saving model to mymodel2.h5\n","276/276 - 218s - loss: 0.1946 - accuracy: 0.9242 - val_loss: 0.2984 - val_accuracy: 0.8837 - 218s/epoch - 789ms/step\n","Epoch 7/50\n","\n","Epoch 7: val_loss improved from 0.29840 to 0.26892, saving model to mymodel2.h5\n","276/276 - 226s - loss: 0.1844 - accuracy: 0.9254 - val_loss: 0.2689 - val_accuracy: 0.9006 - 226s/epoch - 819ms/step\n","Epoch 8/50\n","\n","Epoch 8: val_loss did not improve from 0.26892\n","276/276 - 219s - loss: 0.1582 - accuracy: 0.9372 - val_loss: 0.2987 - val_accuracy: 0.8858 - 219s/epoch - 795ms/step\n","Epoch 9/50\n","\n","Epoch 9: val_loss did not improve from 0.26892\n","276/276 - 220s - loss: 0.1597 - accuracy: 0.9363 - val_loss: 0.3785 - val_accuracy: 0.8531 - 220s/epoch - 796ms/step\n","Epoch 10/50\n","\n","Epoch 10: val_loss did not improve from 0.26892\n","276/276 - 220s - loss: 0.1579 - accuracy: 0.9379 - val_loss: 0.3421 - val_accuracy: 0.8689 - 220s/epoch - 796ms/step\n","Epoch 11/50\n","\n","Epoch 11: val_loss did not improve from 0.26892\n","276/276 - 219s - loss: 0.1449 - accuracy: 0.9428 - val_loss: 0.3154 - val_accuracy: 0.8869 - 219s/epoch - 794ms/step\n","Epoch 12/50\n","\n","Epoch 12: val_loss did not improve from 0.26892\n","276/276 - 219s - loss: 0.1527 - accuracy: 0.9423 - val_loss: 0.3152 - val_accuracy: 0.8901 - 219s/epoch - 794ms/step\n","Epoch 13/50\n","\n","Epoch 13: val_loss did not improve from 0.26892\n","276/276 - 216s - loss: 0.1428 - accuracy: 0.9428 - val_loss: 0.2784 - val_accuracy: 0.8879 - 216s/epoch - 784ms/step\n","Epoch 14/50\n","\n","Epoch 14: val_loss did not improve from 0.26892\n","276/276 - 217s - loss: 0.1358 - accuracy: 0.9461 - val_loss: 0.3043 - val_accuracy: 0.8911 - 217s/epoch - 786ms/step\n","Epoch 15/50\n","\n","Epoch 15: val_loss did not improve from 0.26892\n","276/276 - 216s - loss: 0.1256 - accuracy: 0.9506 - val_loss: 0.4971 - val_accuracy: 0.8256 - 216s/epoch - 784ms/step\n","Epoch 16/50\n","\n","Epoch 16: val_loss did not improve from 0.26892\n","276/276 - 218s - loss: 0.1204 - accuracy: 0.9548 - val_loss: 0.3920 - val_accuracy: 0.8647 - 218s/epoch - 790ms/step\n","Epoch 17/50\n","\n","Epoch 17: val_loss did not improve from 0.26892\n","276/276 - 217s - loss: 0.1134 - accuracy: 0.9565 - val_loss: 0.4127 - val_accuracy: 0.8636 - 217s/epoch - 787ms/step\n","Epoch 18/50\n","\n","Epoch 18: val_loss did not improve from 0.26892\n","276/276 - 217s - loss: 0.1184 - accuracy: 0.9536 - val_loss: 0.5211 - val_accuracy: 0.8182 - 217s/epoch - 785ms/step\n","Epoch 19/50\n","\n","Epoch 19: val_loss did not improve from 0.26892\n","276/276 - 242s - loss: 0.1097 - accuracy: 0.9573 - val_loss: 0.4069 - val_accuracy: 0.8636 - 242s/epoch - 878ms/step\n","Epoch 20/50\n","\n","Epoch 20: val_loss did not improve from 0.26892\n","276/276 - 238s - loss: 0.1093 - accuracy: 0.9578 - val_loss: 0.3818 - val_accuracy: 0.8668 - 238s/epoch - 862ms/step\n","Epoch 21/50\n","\n","Epoch 21: val_loss did not improve from 0.26892\n","276/276 - 218s - loss: 0.1185 - accuracy: 0.9534 - val_loss: 0.3980 - val_accuracy: 0.8584 - 218s/epoch - 790ms/step\n","Epoch 22/50\n","\n","Epoch 22: val_loss did not improve from 0.26892\n","276/276 - 216s - loss: 0.1131 - accuracy: 0.9577 - val_loss: 0.4901 - val_accuracy: 0.8436 - 216s/epoch - 784ms/step\n","Epoch 23/50\n","\n","Epoch 23: val_loss did not improve from 0.26892\n","276/276 - 221s - loss: 0.1109 - accuracy: 0.9536 - val_loss: 0.3940 - val_accuracy: 0.8763 - 221s/epoch - 802ms/step\n","Epoch 24/50\n","\n","Epoch 24: val_loss did not improve from 0.26892\n","276/276 - 222s - loss: 0.1028 - accuracy: 0.9587 - val_loss: 0.3242 - val_accuracy: 0.8996 - 222s/epoch - 805ms/step\n","Epoch 25/50\n","\n","Epoch 25: val_loss did not improve from 0.26892\n","276/276 - 237s - loss: 0.0984 - accuracy: 0.9615 - val_loss: 0.3640 - val_accuracy: 0.8689 - 237s/epoch - 858ms/step\n","Epoch 26/50\n","\n","Epoch 26: val_loss did not improve from 0.26892\n","276/276 - 218s - loss: 0.1077 - accuracy: 0.9563 - val_loss: 0.3604 - val_accuracy: 0.8901 - 218s/epoch - 791ms/step\n","Epoch 27/50\n","\n","Epoch 27: val_loss did not improve from 0.26892\n","276/276 - 218s - loss: 0.0863 - accuracy: 0.9670 - val_loss: 0.4305 - val_accuracy: 0.8689 - 218s/epoch - 789ms/step\n","Epoch 28/50\n","\n","Epoch 28: val_loss did not improve from 0.26892\n","276/276 - 220s - loss: 0.0988 - accuracy: 0.9595 - val_loss: 0.3550 - val_accuracy: 0.8879 - 220s/epoch - 796ms/step\n","Epoch 29/50\n","\n","Epoch 29: val_loss did not improve from 0.26892\n","276/276 - 220s - loss: 0.0987 - accuracy: 0.9602 - val_loss: 0.4059 - val_accuracy: 0.8742 - 220s/epoch - 797ms/step\n","Epoch 30/50\n","\n","Epoch 30: val_loss did not improve from 0.26892\n","276/276 - 219s - loss: 0.0951 - accuracy: 0.9644 - val_loss: 0.4564 - val_accuracy: 0.8721 - 219s/epoch - 792ms/step\n","Epoch 31/50\n","\n","Epoch 31: val_loss did not improve from 0.26892\n","276/276 - 217s - loss: 0.0841 - accuracy: 0.9683 - val_loss: 0.3698 - val_accuracy: 0.8879 - 217s/epoch - 787ms/step\n","Epoch 32/50\n","\n","Epoch 32: val_loss did not improve from 0.26892\n","276/276 - 219s - loss: 0.1025 - accuracy: 0.9603 - val_loss: 0.3715 - val_accuracy: 0.8932 - 219s/epoch - 792ms/step\n","Epoch 33/50\n","\n","Epoch 33: val_loss did not improve from 0.26892\n","276/276 - 218s - loss: 0.0896 - accuracy: 0.9643 - val_loss: 0.3858 - val_accuracy: 0.9006 - 218s/epoch - 791ms/step\n","Epoch 34/50\n","\n","Epoch 34: val_loss did not improve from 0.26892\n","276/276 - 218s - loss: 0.0808 - accuracy: 0.9705 - val_loss: 0.3698 - val_accuracy: 0.8848 - 218s/epoch - 792ms/step\n","Epoch 35/50\n","\n","Epoch 35: val_loss did not improve from 0.26892\n","276/276 - 219s - loss: 0.0902 - accuracy: 0.9662 - val_loss: 0.4236 - val_accuracy: 0.8827 - 219s/epoch - 794ms/step\n","Epoch 36/50\n","\n","Epoch 36: val_loss did not improve from 0.26892\n","276/276 - 221s - loss: 0.0835 - accuracy: 0.9654 - val_loss: 0.4890 - val_accuracy: 0.8562 - 221s/epoch - 801ms/step\n","Epoch 37/50\n","\n","Epoch 37: val_loss did not improve from 0.26892\n","276/276 - 219s - loss: 0.0763 - accuracy: 0.9748 - val_loss: 0.4289 - val_accuracy: 0.8774 - 219s/epoch - 793ms/step\n","Epoch 38/50\n","\n","Epoch 38: val_loss did not improve from 0.26892\n","276/276 - 217s - loss: 0.0808 - accuracy: 0.9678 - val_loss: 0.4811 - val_accuracy: 0.8689 - 217s/epoch - 785ms/step\n","Epoch 39/50\n","\n","Epoch 39: val_loss did not improve from 0.26892\n","276/276 - 237s - loss: 0.0831 - accuracy: 0.9706 - val_loss: 0.3989 - val_accuracy: 0.8953 - 237s/epoch - 858ms/step\n","Epoch 40/50\n","\n","Epoch 40: val_loss did not improve from 0.26892\n","276/276 - 225s - loss: 0.0829 - accuracy: 0.9668 - val_loss: 0.4085 - val_accuracy: 0.8848 - 225s/epoch - 815ms/step\n","Epoch 41/50\n","\n","Epoch 41: val_loss did not improve from 0.26892\n","276/276 - 218s - loss: 0.0794 - accuracy: 0.9720 - val_loss: 0.3826 - val_accuracy: 0.8890 - 218s/epoch - 789ms/step\n","Epoch 42/50\n","\n","Epoch 42: val_loss did not improve from 0.26892\n","276/276 - 218s - loss: 0.0756 - accuracy: 0.9721 - val_loss: 0.5660 - val_accuracy: 0.8552 - 218s/epoch - 789ms/step\n","Epoch 43/50\n","\n","Epoch 43: val_loss did not improve from 0.26892\n","276/276 - 220s - loss: 0.0782 - accuracy: 0.9719 - val_loss: 0.4799 - val_accuracy: 0.8721 - 220s/epoch - 797ms/step\n","Epoch 44/50\n","\n","Epoch 44: val_loss did not improve from 0.26892\n","276/276 - 239s - loss: 0.0822 - accuracy: 0.9704 - val_loss: 0.4190 - val_accuracy: 0.8953 - 239s/epoch - 865ms/step\n","Epoch 45/50\n","\n","Epoch 45: val_loss did not improve from 0.26892\n","276/276 - 218s - loss: 0.0679 - accuracy: 0.9736 - val_loss: 0.7361 - val_accuracy: 0.8203 - 218s/epoch - 792ms/step\n","Epoch 46/50\n","\n","Epoch 46: val_loss did not improve from 0.26892\n","276/276 - 219s - loss: 0.0765 - accuracy: 0.9717 - val_loss: 0.3967 - val_accuracy: 0.8975 - 219s/epoch - 795ms/step\n","Epoch 47/50\n","\n","Epoch 47: val_loss did not improve from 0.26892\n","276/276 - 236s - loss: 0.0694 - accuracy: 0.9761 - val_loss: 0.3735 - val_accuracy: 0.8858 - 236s/epoch - 856ms/step\n","Epoch 48/50\n"]},{"output_type":"error","ename":"UnknownError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-94912e471bce>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m model_history = model.fit(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) UNKNOWN:  FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/Dataset/AI Gen/4978.jpg'\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 267, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\", line 902, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\", line 1049, in generator_fn\n    yield x[i]\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\", line 370, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/image_utils.py\", line 422, in load_img\n    with open(path, \"rb\") as f:\n\nFileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/Dataset/AI Gen/4978.jpg'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[gradient_tape/binary_crossentropy/logistic_loss/mul/Shape_1/_6]]\n  (1) UNKNOWN:  FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/Dataset/AI Gen/4978.jpg'\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 267, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\", line 902, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\", line 1049, in generator_fn\n    yield x[i]\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\", line 370, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/image_utils.py\", line 422, in load_img\n    with open(path, \"rb\") as f:\n\nFileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/Dataset/AI Gen/4978.jpg'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_2829]"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","\n","training_set_path = \"/content/drive/MyDrive/test/Real\"  # Replace with the actual path to your training set\n","\n","# Iterate over all files in the training set directory\n","for filename in os.listdir(training_set_path):\n","    image_path = os.path.join(training_set_path, filename)\n","\n","    # Try to open the image using OpenCV\n","    try:\n","        image = cv2.imread(image_path)\n","        if image is None:\n","            # Image cannot be opened\n","            print(f\"Deleting image: {filename}\")\n","            os.remove(image_path)\n","    except Exception as e:\n","        # An error occurred while opening the image\n","        print(f\"Error opening image {filename}: {str(e)}\")"],"metadata":{"id":"uM-bqcOD9t1S","executionInfo":{"status":"ok","timestamp":1687971913723,"user_tz":-330,"elapsed":16099,"user":{"displayName":"Gaja Lakshmi Gedala","userId":"08948831165546628882"}}},"execution_count":19,"outputs":[]}]}